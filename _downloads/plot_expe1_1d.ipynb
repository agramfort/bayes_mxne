{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n============================================================\nExperiment 1: Variable feature correlation with a 1d problem\n============================================================\n\nThis example aims to illustrate the difficulty to recover sources\nin the presence of stronger correlation between the columns of the\ngain matrix G.\n\nFirst, we construct G as a random matrix in the following way: its rows are\ndrawn from a Gaussian distribution with zero mean and a block-diagonal\ncovariance matrix C = block_diag(C1, C2), where (C1)i,j = 0.5|i\u2212j|,\n(C2)i,j = 0.95|i\u2212j|, i, j = 1,...,10. Then, each column is normalized to have\nunit l2 norm. First figure illustrates the set-up (See Figure 1 in paper [1]).\n\nX is a sparse vector with a value of 1 at index 4 and 14.\nWe want to illustrate that due to the asymmetry in the design, the correct\nrecovery of the source at index 14 is more difcult due to the stronger\ncorrelation in the second block of columns.\n\nWe generate M by adding Gaussian white noise with standard deviation\nequal to 0.2 max(GX). We first run the MM algorithm 1 using a uniform\ninitializzation, i.e. w = ones(n_features), with lambda\nset to 0.2 lambda_max. lambda_max is the smallest regularization value for\nwhich no source is found as active using an l2,1 regularization (Ndiaye et al\n2015, Strohmeier et al 2016). With the MNE implementation of the MM\nsolver.\n\nIt does not recover an X supported at locations 4 and 14, i.e. it is not able\nto locate the sources correctly. Then, we run algorithm 3 from paper\nthe same settings for the majorization-minimization (MM) algorithm as\nbefore to obtain chains of posterior samples, and the corresponding posterior\nmodes. We also show that one can find a better local minimum with algorithm 3.\n\nWe finally cluster the modes based on their spatial support. This reveals\nmultiple modes in the posterior. Figure 2 depicts the spatial support of the\nmodes listed based on the relative frequency with which they were found.\nIt reveals that, indeed, there is a larger uncertainty in the location of the\nsecond source and that in this scenario, the support\nof the mode which is found most often coincides with that of the true solution.\n\nThe example aims to replicate experiment 1 in the paper and the figure 1 and 2.\n\nReference:\n\n[1] Bekhti, Y., Lucka, F., Salmon, J., & Gramfort, A. (2018). A hierarchical\nBayesian perspective on majorization-minimization for non-convex sparse\nregression: application to M/EEG source imaging. Inverse Problems, Volume 34,\nNumber 8.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Yousra Bekhti <yousra.bekhti@gmail.com>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Felix Lucka <f.lucka@ucl.ac.uk>\n\n# License: BSD (3-clause)\n\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.linalg.special_matrices import toeplitz\nimport matplotlib.pyplot as plt\n\nfrom mne.inverse_sparse.mxne_optim import iterative_mixed_norm_solver\nfrom bayes_mxne import mm_mixed_norm_bayes\nfrom bayes_mxne.utils import unique_rows\n\nprint(__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Construction of simulated data\n------------------------------\n\nFirst we define the problem size and the location of the active sources.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_features = 20\nn_samples = 10\nn_times = 1\nlambda_percent = 20.\nK = 1000\n\nX_true = np.zeros((n_features, n_times))\n# Active sources at indices 10 and 30\nX_true[4, :] = 1.\nX_true[14, :] = 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Construction of a covariance matrix\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(0)\n# Set the correlation of each simulated source\ncorr = [0.5, 0.95]\ncov = []\nfor c in corr:\n    this_cov = toeplitz(c ** np.arange(0, n_features // len(corr)))\n    cov.append(this_cov)\n\ncov = np.array(linalg.block_diag(*cov))\n\nplt.matshow(cov)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.title('True Covariance')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulation of the design matrix / forward operator\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "G = rng.multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n\nplt.matshow(G.T.dot(G))\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.title(\"Feature covariance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulation of the data with some noise\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "M = G.dot(X_true)\nM += 0.2 * np.max(np.abs(M)) * rng.randn(n_samples, n_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the regularization parameter and run the MM solver\n---------------------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lambda_max = np.max(np.linalg.norm(np.dot(G.T, M), axis=1))\nlambda_ref = lambda_percent / 100. * lambda_max\n\nX_mm, active_set_mm, E = \\\n    iterative_mixed_norm_solver(M, G, lambda_ref, n_mxne_iter=10)\n\npobj_l2half_X_mm = E[-1]\n\nprint(\"Found support: %s\" % np.where(active_set_mm)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the solver\n--------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Xs, active_sets, lpp_samples, lpp_Xs, pobj_l2half_Xs = \\\n    mm_mixed_norm_bayes(M, G, lambda_ref, K=K)\n\n# Plot if we found better local minima then the first result found be the\nplt.figure()\nplt.hist(pobj_l2half_Xs, bins=20, label=\"Modes obj.\")\nplt.axvline(pobj_l2half_X_mm, label=\"MM obj.\")\nplt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the frequency of the supports\n----------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unique_supports = unique_rows(active_sets)\nn_modes = len(unique_supports)\n\nprint('Number of modes identified: %d' % n_modes)\n\n# Now get frequency of each support\nfrequency = np.empty(len(unique_supports))\nfor k, support in enumerate(unique_supports):\n    frequency[k] = np.mean(np.sum(active_sets !=\n                                  support[np.newaxis, :], axis=1) == 0)\n\n# Sort supports by frequency\norder = np.argsort(frequency)[::-1]\nunique_supports = unique_supports[order]\nfrequency = frequency[order]\n\n# Plot support frequencies in a colorful way\nC = unique_supports * np.arange(n_features, dtype=float)[np.newaxis, :]\nC[C == 0] = np.nan\nplt.matshow(C, cmap=plt.cm.spectral)\nplt.xticks(range(20))\nplt.yticks(range(n_modes), [\"%2.1f%%\" % (100 * f,) for f in frequency])\nplt.ylabel(\"Support Freqency\")\nplt.xlabel('Features')\nplt.grid('on', alpha=0.5)\nplt.gca().xaxis.set_ticks_position('bottom')\n\n# Plot a matrix which shows in its (i, j)th entry the frequency with which\n# locations i and j are simultaneously found active in a mode estimate.\nas_cov = np.dot(active_sets.T, active_sets.astype(float)) / K\n\n# Active set covariance\nplt.matshow(as_cov)\nplt.clim([0, 1])\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.title('Active set covariance')\nplt.colorbar()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}