{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n=========================================================\nExperiment 2: Show relative frequencies with a 1d problem\n=========================================================\n\nWhile the posterior mode whose support coincided with that of the true\nsolution was also found with the highest relative frequency, it is not clear\nwhether this frequency is a reliable indication of the mode\u2019s true relative\nposterior mass. In general, this question is difficult to examine for high\ndimensional problems. Nonetheless, here we constructed an example to at least\nshow that the frequencies are consistent: we now draw the rows of a 10 \u00d7 10\nmatrix G from a Gaussian distribution with zero mean and the covariance matrix\n(Toeplitz with 0.95 correlation).\n\nThen, we set G = [G, G], i.e. the first and last 10 columns of G are exactly\nthe same. This means that the regression problem (1) and the posterior\ndistribution are invariant with respect to switching the first and last 10\nentries. Every mode has a corresponding copy \u2018on the other side\u2019, which should\nbe found with the same relative frequency.\n\nThe example aims to replicate experiment 2 in the paper and the figure 4.\n\nReference:\n\n[1] Bekhti, Y., Lucka, F., Salmon, J., & Gramfort, A. (2018). A hierarchical\nBayesian perspective on majorization-minimization for non-convex sparse\nregression: application to M/EEG source imaging. Inverse Problems, Volume 34,\nNumber 8.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Yousra Bekhti <yousra.bekhti@gmail.com>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Felix Lucka <f.lucka@ucl.ac.uk>\n\n# License: BSD (3-clause)\n\nimport numpy as np\nfrom scipy.linalg.special_matrices import toeplitz\nimport matplotlib.pyplot as plt\n\nfrom mne.inverse_sparse.mxne_optim import iterative_mixed_norm_solver\nfrom bayes_mxne import mm_mixed_norm_bayes\nfrom bayes_mxne.utils import unique_rows\n\nprint(__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Construction of simulated data\n------------------------------\n\nFirst we define the problem size and the location of the active sources.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_features = 20\nn_samples = 10\nn_times = 1\nlambda_percent = 50.\nK = 5000\n\nX_true = np.zeros((n_features, n_times))\n# Active sources at indices 10 and 30\nX_true[4, :] = 1.\nX_true[14, :] = 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Construction of a covariance matrix\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(0)\n# Set the correlation of each simulated source\ncorr = 0.95\ncov = toeplitz(corr ** np.arange(0, n_features // 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulation of the design matrix / forward operator\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "G = rng.multivariate_normal(np.zeros(len(cov)), cov, size=n_samples)\nG = np.concatenate((G, G), axis=1)\nG /= np.linalg.norm(G, axis=0)[np.newaxis, :]  # normalize columns\n\nplt.matshow(G.T.dot(G))\nplt.title(\"Feature covariance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulation of the data with some noise\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "M = G.dot(X_true)\nM += 0.2 * np.max(np.abs(M)) * rng.randn(n_samples, n_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the regularization parameter and run the solver\n------------------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lambda_max = np.max(np.linalg.norm(np.dot(G.T, M), axis=1))\nlambda_ref = lambda_percent / 100. * lambda_max\n\nX_mm, active_set_mm, _ = \\\n    iterative_mixed_norm_solver(M, G, lambda_ref, n_mxne_iter=10)\n\nprint(\"Found support: %s\" % np.where(active_set_mm)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the solver\n--------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Xs, active_sets, lpp_samples, lpp_Xs, pobj_l2half_Xs = \\\n    mm_mixed_norm_bayes(M, G, lambda_ref, K=K)\n\n# we plot the log posterior probability to check when the sampler reached\n# its statinary phase\nplt.figure()\nplt.plot(lpp_samples, label='log post. prob. chain samples')\nplt.plot(lpp_Xs, label='log post. prob. Xs (full MAP)')\nplt.xlabel('Samples')\nplt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the frequency of the supports\n----------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unique_supports = unique_rows(active_sets)\nn_modes = len(unique_supports)\n\nprint('Number of modes identified: %d' % n_modes)\n\n# Now get frequency of each support\nfrequency = np.empty(len(unique_supports))\nfor k, support in enumerate(unique_supports):\n    frequency[k] = np.mean(np.sum(active_sets !=\n                                  support[np.newaxis, :], axis=1) == 0)\n\n# Sort supports by frequency\norder = np.argsort(frequency)[::-1]\nunique_supports = unique_supports[order]\nfrequency = frequency[order]\n\n# Plot support frequencies in a colorful way\nC = unique_supports * np.arange(n_features, dtype=float)[np.newaxis, :]\nC[C == 0] = np.nan\nplt.matshow(C, cmap=plt.cm.spectral)\nplt.xticks(range(20))\nplt.yticks(range(n_modes), [\"%2.1f%%\" % (100 * f,) for f in frequency])\nplt.ylabel(\"Support Freqency\")\nplt.xlabel('Features')\nplt.grid('on', alpha=0.5)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}